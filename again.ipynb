{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1f8843",
      "metadata": {
        "id": "6d1f8843",
        "outputId": "ef4191fb-9c95-46fa-d4e1-fd04639480dd",
        "colab": {
          "referenced_widgets": [
            "a05b6e6162a64909a5bcdabc5ffafca7"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a05b6e6162a64909a5bcdabc5ffafca7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de37c8a",
      "metadata": {
        "id": "0de37c8a"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fa97763",
      "metadata": {
        "id": "7fa97763"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3bae277",
      "metadata": {
        "id": "c3bae277"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5263c9d",
      "metadata": {
        "id": "f5263c9d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, GPTQConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e194ee39",
      "metadata": {
        "id": "e194ee39"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "935921b5",
      "metadata": {
        "id": "935921b5"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60756235",
      "metadata": {
        "id": "60756235"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ddb18d",
      "metadata": {
        "id": "52ddb18d"
      },
      "outputs": [],
      "source": [
        "def generate_train():\n",
        "    data = load_dataset(\"ananyarn/Algorithm_and_Python_Source_Code\", streaming=True, split=\"train\", download_mode=\"force_redownload\")\n",
        "    samp_total = 10000\n",
        "    valid_p = 0.1\n",
        "    samp_train=int(samp_total*(1-valid_p))\n",
        "    count = 0\n",
        "    for sample in iter(data):\n",
        "        if count >= samp_train:\n",
        "            break\n",
        "        q = sample['Algorithm']\n",
        "        start = q.find(\"1\")\n",
        "        question = q[start:].strip()\n",
        "        answer = sample['output'].strip()\n",
        "        nf = f'<s>[INST]{question}[/INST] ```python\\n{answer}```</s>'\n",
        "        output_token = tokenizer(nf)\n",
        "        yield{'text':nf}\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d36f475e",
      "metadata": {
        "id": "d36f475e"
      },
      "outputs": [],
      "source": [
        "def generate_validation():\n",
        "    data = load_dataset(\"ananyarn/Algorithm_and_Python_Source_Code\", streaming=True, split=\"train\", download_mode=\"force_redownload\")\n",
        "    samp_total = 10000\n",
        "    valid_p = 0.1\n",
        "    samp_train=int(samp_total*(1-valid_p))\n",
        "    count = 0\n",
        "    for sample in iter(data):\n",
        "        if count < samp_train:\n",
        "            count+=1\n",
        "            continue\n",
        "        if count >= samp_total:\n",
        "            break\n",
        "        q = sample['Algorithm']\n",
        "        start = q.find(\"1\")\n",
        "        question = q[start:].strip()\n",
        "        answer = sample['output'].strip()\n",
        "        nf = f'<s>[INST]{question}[/INST] ```python\\n{answer}```</s>'\n",
        "        output_token = tokenizer(nf)\n",
        "        yield{'text':nf}\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2779a8",
      "metadata": {
        "id": "2a2779a8"
      },
      "outputs": [],
      "source": [
        "config_details=GPTQConfig(bits=4, use_exllama=False, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab82807c",
      "metadata": {
        "id": "ab82807c"
      },
      "outputs": [],
      "source": [
        "dev_map = \"auto\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d76215a",
      "metadata": {
        "id": "0d76215a",
        "outputId": "b61c6358-34bf-4ae1-8a07-093b12c02b19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\", quantization_config=config_details, device_map=dev_map, token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1f7e55",
      "metadata": {
        "id": "bd1f7e55",
        "outputId": "7b8295f0-048c-4e15-ceaf-760e1fde8cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "          (k_proj): QuantLinear()\n",
            "          (o_proj): QuantLinear()\n",
            "          (q_proj): QuantLinear()\n",
            "          (v_proj): QuantLinear()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (act_fn): SiLU()\n",
            "          (down_proj): QuantLinear()\n",
            "          (gate_proj): QuantLinear()\n",
            "          (up_proj): QuantLinear()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa77c732",
      "metadata": {
        "id": "aa77c732"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4344c85",
      "metadata": {
        "id": "f4344c85"
      },
      "outputs": [],
      "source": [
        "model.config.pretraining_tp=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1453fadd",
      "metadata": {
        "id": "1453fadd"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fca69a",
      "metadata": {
        "id": "d4fca69a"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2604d81",
      "metadata": {
        "id": "f2604d81"
      },
      "outputs": [],
      "source": [
        "model.config.window = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1391865b",
      "metadata": {
        "id": "1391865b"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(lora_alpha=16, lora_dropout=0.1, r=32, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\",],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64695a9",
      "metadata": {
        "id": "f64695a9"
      },
      "outputs": [],
      "source": [
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6e6c4d",
      "metadata": {
        "id": "5f6e6c4d"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(output_dir=\"./get_python\", per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=4, optim=\"paged_adamw_32bit\", save_steps=50, max_grad_norm=0.3, weight_decay=0.01, learning_rate=2e-5, lr_scheduler_type=\"constant\", evaluation_strategy=\"steps\", logging_steps=50, num_train_epochs=1, max_steps=250, warmup_ratio=0.03, group_by_length=True, fp16=False, push_to_hub=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb2c570",
      "metadata": {
        "id": "1bb2c570"
      },
      "outputs": [],
      "source": [
        "train_generator=Dataset.from_generator(generate_train)\n",
        "val_generator=Dataset.from_generator(generate_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77479ce",
      "metadata": {
        "id": "c77479ce"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side=\"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf607fb2",
      "metadata": {
        "id": "bf607fb2"
      },
      "outputs": [],
      "source": [
        "trainer=SFTTrainer(model=model, train_dataset=train_generator, eval_dataset=val_generator, peft_config=peft_config, dataset_text_field=\"text\", args=training_arguments, tokenizer=tokenizer, packing=False, max_seq_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a124cb5",
      "metadata": {
        "id": "1a124cb5",
        "outputId": "18e237cf-7742-4469-b245-03e716cf1be0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 1:24:50, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.832600</td>\n",
              "      <td>0.704576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.640400</td>\n",
              "      <td>0.608044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.577100</td>\n",
              "      <td>0.570125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.563700</td>\n",
              "      <td>0.566240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.552000</td>\n",
              "      <td>0.571804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=250, training_loss=0.6331560745239257, metrics={'train_runtime': 5106.6394, 'train_samples_per_second': 0.783, 'train_steps_per_second': 0.049, 'total_flos': 1918182537658368.0, 'train_loss': 0.6331560745239257, 'epoch': 0.44})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e52e49f",
      "metadata": {
        "id": "0e52e49f",
        "outputId": "10081a86-275c-4c79-d399-55a95e645f6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cs-ai-04/anaconda3/envs/ml/lib/python3.10/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/ananyarn/get_python/commit/44b7488d0ba7aa2ce64929362428318e00e6b22d', commit_message='End of training', commit_description='', oid='44b7488d0ba7aa2ce64929362428318e00e6b22d', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}